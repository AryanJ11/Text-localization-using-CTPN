{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regr_loss(nn.Module):\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        super(regr_loss, self).__init__()\n",
    "        \n",
    "        self.device=device\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "\n",
    "        cls=target[0, :, 0]\n",
    "        regr=target[0, :, 1:]\n",
    "        regr_keep=(cls == 1).nonzero(as_tuple=False)[:, 0]\n",
    "        regr_true=regr[regr_keep]\n",
    "        regr_pred=inp[0][regr_keep]\n",
    "        diff=torch.abs(regr_true-regr_pred)\n",
    "        \n",
    "        less_one=(diff<1.0).float()\n",
    "        \n",
    "        loss = less_one*0.5*(diff**2) + torch.abs(1-less_one)*(diff-0.5)\n",
    "        \n",
    "        loss = torch.sum(loss, 1)\n",
    "        loss = torch.mean(loss) if loss.numel()>0 else torch.tensor(0.0)\n",
    "\n",
    "        return loss.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cls_loss(nn.Module):\n",
    "    \n",
    "    def __init__(self,device):\n",
    "        super(cls_loss, self).__init__()\n",
    "      \n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        \n",
    "        y_true = target[0][0]\n",
    "        cls_keep = (y_true != -1).nonzero(as_tuple=False)[:, 0]\n",
    "        cls_true = y_true[cls_keep].long()\n",
    "        cls_pred = inp[0][cls_keep]\n",
    "        \n",
    "        loss = F.nll_loss(F.log_softmax(cls_pred, dim=-1), cls_true)  # negative-log-likelihood loss\n",
    "        loss = torch.mean(loss) if loss.numel() > 0 else torch.tensor(0.0) \n",
    "        \n",
    "        return loss.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp, out, kernel, stride=1, padding=0, activation=True, bn=False, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv=nn.Conv2d(inp, out, kernel, stride=stride, padding=padding, bias=bias)\n",
    "        self.norm=nn.BatchNorm2d(out, eps=1e-5, momentum=0.01, affine=True)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.rel=activation\n",
    "        self.bn=bn\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.bn==True:\n",
    "            x=self.norm(x)\n",
    "        \n",
    "        if self.rel==True:\n",
    "            x=self.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTPN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        base_model=models.vgg16(pretrained=True)    \n",
    "        layers=list(base_model.features)[:-1]\n",
    "        self.base_layers=nn.Sequential(*layers)  # block5_conv3 output\n",
    "        \n",
    "        self.conv1=conv_block(512, 512, 3, 1, 1) #region-proposal network layer\n",
    "        self.recurrent=nn.LSTM(512,128, bidirectional=True, batch_first=True)\n",
    "        self.conv2=conv_block(256, 512, 1, 1)\n",
    "        \n",
    "        self.cls=conv_block(512, 10*2, 1, 1, activation=False)\n",
    "        self.regr=conv_block(512, 10*2, 1, 1)\n",
    "        self.refine=conv_block(512, 10, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x=self.base_layers(x)\n",
    "        \n",
    "        # rpn\n",
    "        x=self.conv1(x)\n",
    "\n",
    "        x1=x.permute(0,2,3,1).contiguous()\n",
    "        b=x1.size() \n",
    "        x1=x1.view(b[0]*b[1], b[2], b[3])\n",
    "\n",
    "        x2,_=self.recurrent(x1)\n",
    "\n",
    "        xsz=x.size()\n",
    "        x3=x2.view(xsz[0], xsz[2], xsz[3], 256)\n",
    "\n",
    "        x3=x3.permute(0,3,1,2).contiguous()\n",
    "        x3=self.conv2(x3)\n",
    "        x=x3\n",
    "\n",
    "        cls=self.cls(x)\n",
    "        regr=self.regr(x)\n",
    "        refine=self.refine(x)\n",
    "\n",
    "        cls=cls.permute(0,2,3,1).contiguous()\n",
    "        regr=regr.permute(0,2,3,1).contiguous()\n",
    "        refine=refine.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        cls=cls.view(cls.size(0), cls.size(1)*cls.size(2)*10, 2)\n",
    "        regr=regr.view(regr.size(0), regr.size(1)*regr.size(2)*10, 2)\n",
    "        refine=refine.view(refine.size(0), refine.size(1)*refine.size(2)*10, 1)\n",
    "\n",
    "        return cls, regr, refine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
